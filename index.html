<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>lihan.io</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="lihan.io">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="lihan.io">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="lihan.io">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="lihan.io" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/img/test.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">李涵</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
						<li>个人简介</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/tags/爬虫/">爬虫</a></li>
				        
							<li><a href="/tags/机器学习/">机器学习</a></li>
				        
							<li><a href="/tags/图像处理/">图像处理</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/Lihhhan" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="http://weibo.com/3805057478/profile?is_all=1" title="weibo">weibo</a>
					        
								<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/smilehua-xia" title="zhihu">zhihu</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/图像处理/" style="font-size: 15px;">图像处理</a> <a href="/tags/天气分类/" style="font-size: 10px;">天气分类</a> <a href="/tags/数值分析/" style="font-size: 10px;">数值分析</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a> <a href="/tags/爬虫/" style="font-size: 10px;">爬虫</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">大连理工大学12级本科生，打算本校读研,图像识别方向，学生狗, web后端小菜鸡～</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">李涵</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="/img/test.jpg" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">李涵</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/tags/爬虫/">爬虫</a></li>
		        
					<li><a href="/tags/机器学习/">机器学习</a></li>
		        
					<li><a href="/tags/图像处理/">图像处理</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/Lihhhan" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/3805057478/profile?is_all=1" title="weibo">weibo</a>
			        
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/smilehua-xia" title="zhihu">zhihu</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-浅谈协方差矩阵相关" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/09/浅谈协方差矩阵相关/" class="article-date">
  	<time datetime="2016-11-09T16:47:00.000Z" itemprop="datePublished">2016-11-09</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/09/浅谈协方差矩阵相关/">浅谈协方差矩阵相关</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>最近在实现除雨的过程中使用到了马氏距离的东西，在这其中遇到了一些问题，所以更新一篇有关马氏距离和协方差矩阵相关的内容，我会尽量用最简单的语言结合自己的理解来讲清楚这一系列问题。</p>
<h2 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h2><p>首先我们说一下什么是协方差，我们高中都学过方差，标准差这些，但是这些量都是计算单个随机变量的波动性，譬如一个班级语文成绩的方差，可能表示一个班级的两极分化程度，一个学生各个学科成绩的方差，可能表示他的偏科程度。我们这里说的协方差，是一个衡量两个随机变量之间联系的一个统计值,譬如一个班级数学成绩和物理成绩的协方差，表示数学这门学科和物理这门学科的相关性。我和我同桌小考成绩的协方差，很可能具有某种神秘的相关性，而表现出一个正值。先给出公式:<br>\[ cov(x,y) = E((x-E(x))(y-E(y)))\]
这种形式不太容易理解，可以把这个公式变形为另一个形式：
\[cov(x,y) = E(x・y)-E(x)E(y)\]
可以看出两个随机变量的协方差实际上就是它们的內积减去它们的平均值，事实上，两个向量的內积就是一个向量在另一个向量方向上的投影：
\[x \bullet y = |x||y|cos(\alpha)\]
这个$cos \alpha $所以我们现在可以很形象的理解,两个向量夹角越小，必然越相似，其实就是两个随机变量相似性越大，即协方差越大，如果两个随机变量是同一个随机变量，其实计算他们的协方差就是在计算他们的方差。</p>
<p>回到协方差矩阵上，如果一个高中生学习了10门课程，统计全班所有学生的成绩可以构成一个n*10的矩阵$X$,$ X=(X_1,X_2,…,X_{10})$每一个列向量$ X_i $代表一门学科成绩的随机变量，我们把每两门学科成绩都相互计算一下协方差，就能构成一个协方差矩阵。
\[ C = \left (\begin{matrix}
cov(X_1,X_1)&cov(X_1,X_2)&…&cov(X_1,X_{10}) \\
cov(X_2,X_1)&cov(X_2,X_2)&…&cov(X_2,X_{10}) \\
… \\
cov(X_{i-1},X_1)&cov(X_{i-1},X_2)&…&cov(X_{i-1},X_i) \\
… \\
cov(X_{10},X_1)&cov(X_{10},X_2)&…&cov(X_{10},X_{10}) \\
\end{matrix} \right) \]</p>
协方差矩阵中的$c_{i,j}$就表示第i个特征跟第j个特征的协方差，特别的是它的对角线$c_{i,i}$其实表示的是第i个特征的方差。
<h2 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h2><p>马氏距离由印度数学家马哈拉诺比斯(Mahalanobis)首先提出，所以又称为Mahalanobis Distance,是一种采样协方差来计算两点之间距离的方法。他的公式如下:
\[d_m(u,v) = \sqrt{(u-v)^T \Sigma^{-1} (u-v) }\]
这里的$\Sigma$ 是协方差矩阵,u,v是两个样本点，不是样本集的某两列特征。
<br>我们可以看到在计算马氏距离的时候用到了协方差矩阵的逆，从上述协方差矩阵的介绍中可以看出，因为协方差本身是一种计算两个随机变量之间相关性的值，所以马氏距离是建立在总体样本基础上的一种运算，两个点在不同的总体中，他的马氏距离是不同的。另外，如果这个协方差矩阵的逆是一个单位矩阵，马氏距离等于欧氏距离。</p>
<p>这里的协方差矩阵的逆，实际上是总体样本的协方差矩阵的逆，并不是两个待比较的样本之间的协方差矩阵,如果总体样本数小于样本的维数，很容易算出样本的协方差矩阵为奇异矩阵，并没有逆（..就是被坑在这里了，对两个36维的hog特征计算马氏距离，如果不代入总样本计算协方差矩阵，对两个样本计算得到的协方差矩阵一定是奇异的）。</p>
<p>以上的关于马氏距离的描述有点抽象，我从我的理解来简单解释一下马氏距离的思想，举个不恰当的例子来说，A学生的物理，数学，语文成绩分别是（90, 90, 60），现在B学生的三门成绩为（100,100,60），C学生的三门成绩为（90,90,71），如果直接用欧式距离来计算他们之间的相似程度，我们可以得到A和C之间的欧式距离更小，他们之间的相似度更高，可是如果我们拿到整个班级的三门课成绩，并对计算他们总体的协方差矩阵，可能会发现，全班的物理成绩和数学成绩的协方差很大，具有一定的线性关系，直接对学生的三门课成绩进行欧式距离的计算，笼统的来说，意味着对所谓的“物理，数学”成绩的影响实际上是重复的，但是被放大了。</p>
<p>在网上也看过一部分资料，解释所谓的马氏距离，是对特征向量进行了特征空间的变化，从欧式空间变换到了马氏空间，那么我就顺着这个简单说一下协方差矩阵是怎么做的PCA降维的。</p>
<h2 id="PCA降维"><a href="#PCA降维" class="headerlink" title="PCA降维"></a>PCA降维</h2><p>网上关于PCA降维的介绍很多，我这里只简单介绍一下我所理解的PCA降维。</p>
<p>如果我们现在有一个n维的特征向量的样本，可以计算出他的协方差矩阵C，则C是一个n*n的矩阵。<br>上面说到两个随机变量协方差计算的公式的內积形式是:
\[cov(x,y) = E(xy) - E(x)E(y)\]
\[cov(x,y) = x^T・y - E(x)E(y)\]
我们如果找到一个标准正交基U对x,y进行转换向量空间，我们知道转换向量空间是不会改变向量长度的，所以公式后一项$E(x)E(y)$是不会改变的（实际上如果我们假定我们对特征值x,y做了标准化预处理，则这一项为0）。
\[cov(xU,yU) = (xU)^T・yU - E(x)E(y)\]
\[cov(xU,yU) = U^Tx^T・yU - E(x)E(y)\]
我们对C进行SVD分解得到:
\[C = USV\]<br>其中的U和V都是一个酉阵，也就是一组标准正交基，其实就相当于对我们初始的样本x,y进行了向量空间的转换，S是一个对角阵，除了对角线，其他位置都为0，对角线的所有元素都为C阵的特征值，这一部分内容感兴趣的可以自己参考数值分析的SVD分解部分。我用自己的话来解释一波这个SVD操作，将我们初始的样本进行向量空间转换，由于新的协方差矩阵S除了对角线的方差位，其他协方差位都为0，而且对角线元素越往下越小，也就是说对原来的样本进行换基操作之后，每一维的特征彼此都不再相关，越靠后的特征方差越小（这里我们可以理解为，方差越小包含的信息越少，譬如方差为0，大家这个特征都相等，没有差异性）。然后我们选取酉阵U的前K个向量组成新线性空间的一组基向量，来描述原来的n维特征，也就是把特征数从n维降到了k维，这里舍弃的n-k维，新协方差矩阵中的后n-k维对角线上特征值比较小的特征。</p>
<p>用一句话来总结一下，就是对原来的n维特征做基变换，使n维特征相互间的协方差很小，也就是说特征之间不包含联系，然后我们忽略那些方差比较小的n-k维特征，让原来的n维特征降维到k维特征，对这部分感兴趣的同学可以去看一下底下相关链接[3]，PCA原理，他从基本的向量基变换开始讲起说的很清讲楚。</p>
<blockquote>
<p>相关知识:</p>
<ol>
<li><a href="http://blog.csdn.net/ybdesire/article/details/6270328/" target="_blank" rel="external">详解协方差及协方差矩阵</a></li>
<li><a href="http://www.cnblogs.com/likai198981/p/3167928.html" target="_blank" rel="external">马氏距离的深入理解</a></li>
<li><a href="http://blog.csdn.net/xiaojidan2011/article/details/11595869" target="_blank" rel="external">PCA的数学原理</a></li>
</ol>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/图像处理/">图像处理</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数值分析/">数值分析</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-《Two-Class-Weather-Classification》阅读笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/06/28/《Two-Class-Weather-Classification》阅读笔记/" class="article-date">
  	<time datetime="2016-06-28T11:30:03.000Z" itemprop="datePublished">2016-06-28</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/06/28/《Two-Class-Weather-Classification》阅读笔记/">《Two-Class Weather Classification》阅读笔记</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>《Two-Class Weather Classification》[0]是我最近学习的一篇关于两种天气分类的一篇文章，提出了一种新的针对sunny-cloudy两种天气进行分类的方法。背景就不多说了，我会结合特征提取和分类器的构造两方面来简单说一下我对这篇文章的理解。</p>
<h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>这篇文章针对晴天以及多云的天气主要通过天空，阴影，反射，对比度还有雾霾五种特征来进行分类。对于每张图片，会针对这五个方面构建出一个621维的特征向量\( [ f_{sk} , f_{sh} , f_{re} , f_{co} , f_{ha} ]^T \)，还有一个四维的存在性向量\( [ v_{sk}, v_{sh}, v_{re}, v_{ha}]^T \)（这个存在性向量是用来权衡获得的相应特征对于天气判断的可信度的一个参数）。然后利用这些特征信息进行分类，关于如何利用这些分类信息进行分类，我会在分类器的构造这一块来进行解释。</p>
<ul>
<li>天空<br>天空的情况对于户外天气的判断是一个十分重要的依据。天空特征\(f_{sk}\)是一个256维的特征向量，并且天空特征为四维的存在性向量提供了一维\(v_{sk}\).\(f_{sk}\)的计算是通过从原始图像中抠取天空区域得到的，首先从原始图像中抠取天空区域，然后将非天空区域的像素值置为0，然后对这个处理后的图像计算像素值直方图。0-255的像素值，可以获得一个256维的像素值直方图向量，就把这个向量作为天空特征\(f_sk\), 设天空区域占整个图片中的比例为A，\(v_{sk} = min\{2A,1\}\),关于如何将天空区域从图像中分离出来，文章中是采用了20000个大小为15*15的小块，然后分别计算他们128维的SIFT描述子的特征和3维的HSV颜色空间的像素值，获得一个131维的特征然后用随机森林分类器进行训练然后分类的。作者提供了抠取天空区域这部分工作的代码，网上应该也能找到这方面的工作的代码。</li>
<li>阴影<br>阴影是指在阳光比较强烈的晴天户外照片中才会出现比较明显的影子的特征。和天空特征类似的，需要先在原始图像中把阴影部分检测出来，阴影检测比较复杂，图像中本身就存在的黑色区域往往会被误识别。文章没有提供具体的方法，但是对于阴影检测使用了一个阴影检测工具[1]。由于存在一部分误识别的阴影区域，文章只对检测出来的可信度（这个可信度的计算文章中没有提，应该是检测工具结果里面会返回的一个量）最高的十个区域进行处理，用K-nearest算法分别计算出每个阴影区域在阴影样本集P（这个P是样本集中每个样本中可信度最高的10个阴影区域构成的一个阴影集）上的五个最近邻样本区域，然后这个阴影区域和它的五个最近邻的欧式距离的和，原始图像获得10个可信度最高的阴影区域，计算出10个欧式距离作为一个10维的特征向量\(f_{sh}\),十个可信度最高的阴影区域中最高的那个可信度就是\(v_{sh}\) 。</li>
<li>反射<br>反射是指，强烈的阳光照射在物体上，会在其表面产生白色的反射区域，这种白色区域有中间亮度比较大，边缘亮度小的特点，它把这种反射看做是以物体本身作为背景的图像上做一个白色前景蒙层（也就是alpha matte），也就是说中间前景占比高，随着离中心距离的增大四周的比例逐渐降低。反射特征是通过抠图来找白色反射区域，然后把距离白色区域一定距离的区域划一个曲线，计算曲线内包围像素点的alpha matte的直方图，构造100维的反射特征\(f_{re}\), 如果存在这样的反射则\(v_{re}=1\),反之为0。</li>
<li>对比度<br>这个特征比较容易理解，晴天图像的对比度比较高，色彩饱和度也比较高，其实就是计算出原始图片在HSV颜色空间的色彩饱和度20维的直方图\(P\), 对比度特征\(f_{co}=[ p_{i}/p_{j}, i>j\,\&\&\,p_{i},p_{j}\in P]\)，因为\(p\)是一个20维的向量，\(i,j\in [0-19]\),很容易得到\(f_{co}\)是一个171维的向量,其中由于每个图片都具有对比度,也就是这个值都是可信的，所以可信度向量中没有\(v_{co}\)。</li>
<li>雾霾<br>雾霾利用了暗通道先验原理[2]，首先计算出原始图片的暗通道图，然后把暗通道图resize成512*512的正方形图，并将其分别切块成2*2,4*4,8*8的84个小正方形块，分别计算每个暗通道小块的像素值的中位数，一共是84个中位数，构成一个84维的特征向量\(f_{ha}\),原始图片的暗通道图的中位数就是\(v_{ha}\)。</li>
</ul>
<p>至此我们得到了一个621维的特征向量，还有一个4维的可信度向量。下一步就是根据这些天气特征进行分类。</p>
<h1 id="分类器构造"><a href="#分类器构造" class="headerlink" title="分类器构造"></a>分类器构造</h1><p>由于不是每个图像都包含上述五种天气特征，譬如一副晴天的近景图片没有拍摄到天空,拍摄远处的山脉没有倒影等等，所以不是每种特征都具有所有的可信度，之前我们计算得到了一个四维的可信度向量\( [ v_{sk}, v_{sh}, v_{re}, v_{ha}]^T \),每种特征都可能在图片中存在，也可能不存在，4个特征一共有\(2^4=16\)种可能的图片类型，我们对我们的训练样本利用存在性向量进行聚类，聚类中心有16个，最后得到的结果我们可以理解为，每个聚类存在的特征都是同质的，譬如说第一类是只有天空和阴影特征，第二类只有阴影和反射特征，第三类有所有特征等等。<br>如果顺着传统的思维做下去，我们现在得到了16个子训练集，现在可以对每个子训练集单独进行训练，当拿到一张待分类图片之后，每个子训练集都可以给出一个分类结果，我们只要综合一下这些结果，例如子训练集中心和待分类图片可信度向量越接近的占的权重越高等等，最后给出一个总的结果就行了。$$ h(x,e) = sign[\sum_{i=1}^{M}s(\hat e_{i}, e)\hat h_{i}(x)] \tag{1}$$ 这个公式就是对于上述思想的公式表达,\(x\)和\(e\)分别是621维的特征向量和4维的可信度向量，\(\hat e_{i}\)就是第i个子训练集的聚类中心，\(M\)是聚类的个数，这里是16,\(s\)是一个计算相似度的函数，$$ s(\hat e_i, e)= \frac{exp(-\frac {||\hat e_i-e||_2^2}{2\sigma^2})}{\sum_i^Mexp(-\frac{||\hat e_i-e||_2^2}{2\sigma^2})} \tag{2}$$\(\hat h_i\)表示第i个子训练集的分类结果,结果由1和-1来表示，1为晴天，-1为多云，\(e\)和\(\hat e_i\)的相似度越高\(s\)的值越大，该子训练集给总的预测结果中提供的权重就越大，最后判断这个加权后的所有子训练集给出的结果的正负性来给出总的预测结果。这里的\(\hat h_i(x)\)如果使用最简单的二值分类的svm的分类器[3]的话，就有这样的计算公式：$$\hat h_i(x) = sign(\omega_ix+b_i) \tag{3}$$构建这个分类器也就是计算最大分离面，也就是求这个最小化问题，$$min(\omega^2 + C*\zeta_{i,k})  $$ $$\qquad s.t.\,y_i(\omega_ix+b_i)\geq 1-\zeta_i\tag{4}$$但是文章没用这种方法，它采用了一种各个聚类协作的分类模型，也就是说，各个聚类的分类结果不会因为总的输出结果而改变，所以说\(\hat h_i(x)\)的结果是独立于\(h(x,e)\)的,这样就可以把(3)式代入到(1)中，得到一个共同的预测模型：$$h(x,e) = sign[\sum_{i=1}^{M}s(\hat e_{i}, e)(\omega_ix+b_i)] \tag{5}$$求解这个最优分离平面\(\omega_i\)就是求解这个变形的最小化问题：$$min(\sum_{i=1}^M\omega_i^2+C_1\sum_{i=1}^M\zeta_i+C_2\sum_{t=1}^N\xi_t) $$  $$\qquad s.t. \, yt[\sum^M_is(\hat e_i,e_t)(\omega_ix_t+b_i)]\geq1-\xi_t \tag{6}$$ 这个N指的是训练集的数量,这个最小化问题可以用朗格拉日乘子法计算。这是个类似于SVM的分类模型，这种分类的模型解决了样本集中某些特征不存在，又需要用这些特征进行分类的问题，但是这个模型最大的问题就是只能分类两种天气，因为结果只有-1和+1两种，才可以采用这种各个子类加权，然后再汇总的方法。</p>
<blockquote>
<p>参考</p>
<ol>
<li>Cewu Lu, Di Lin, Jiaya Jia, Chi-Keung Tang. Two-class Weather Classification.In CVPR, 2015.</li>
<li>J.-F. Lalonde, A. Efros, and S. Narasimhan. Detecting ground shadows in outdoor consumer photographs. In ECCV, 2010.</li>
<li><a href="http://blog.csdn.net/zrjust1043/article/details/42965475" target="_blank" rel="external">暗通道先验原理介绍</a>  </li>
<li><a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html" target="_blank" rel="external">svm基础</a></li>
</ol>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/图像处理/">图像处理</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/天气分类/">天气分类</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-用K-Means算法实现知乎用户聚类" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/04/12/用K-Means算法实现知乎用户聚类/" class="article-date">
  	<time datetime="2016-04-12T15:03:50.000Z" itemprop="datePublished">2016-04-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/12/用K-Means算法实现知乎用户聚类/">用K-Means算法实现知乎用户聚类</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近看到利用blog关键字做用户分类的事情，感觉挺神奇的，就用python写了一个基于知乎用户回答内容的用户聚类的小脚本。</p>
<p>主要的流程就是先用爬虫爬取用户回答数据，然后从用户回答中提取关键字，这个关键字分词用的是python的jieba分词库[1]，分词的效率和准确度还可以接受。然后结合一个回答的点赞数，关键字在这个回答的权重以及关键字在不同回答中出现的次数来计算关键字在用户维度的权重，最后得到一个用户/关键字的权重二维矩阵，矩阵的每一行代表,一个用户在不同关键字下的权重的行向量，然后利用Pearson Correlation Score来计算用户相似性，最后用K-Means算法来进行用户聚类。</p>
<p>首先说下爬虫，爬虫唯一麻烦的是登陆，知乎浏览问题是要登陆的，这是登陆知乎的代码</p>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#cookie init</span></span><br><span class="line">    cookies = cookielib.CookieJar()</span><br><span class="line">    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookies))</span><br><span class="line">    urllib2.install_opener(opener)</span><br><span class="line">    c =  urllib2.urlopen(<span class="string">'http://www.zhihu.com'</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">#login</span></span><br><span class="line">    form =  &#123;&#125; </span><br><span class="line">    <span class="keyword">for</span> cookie <span class="keyword">in</span> cookies:</span><br><span class="line">        <span class="keyword">if</span> cookie.name == <span class="string">'_xsrf'</span> :</span><br><span class="line">        form[<span class="string">'_xsrf'</span>] = cookie.value</span><br><span class="line">    form[<span class="string">'password'</span>] = <span class="string">''</span></span><br><span class="line">    form[<span class="string">'email'</span>] = <span class="string">'136081054@qq.com'</span></span><br><span class="line">    form[<span class="string">'remember_me'</span>] = <span class="keyword">False</span></span><br><span class="line">  </span><br><span class="line">    post_data=urllib.urlencode(form)</span><br><span class="line">    headers =&#123;<span class="string">"User-agent"</span>:<span class="string">"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1"</span>&#125;</span><br><span class="line">    req=urllib2.Request(<span class="string">'https://www.zhihu.com/login/email'</span>,post_data,headers)</span><br><span class="line">    content=json.load(opener.open(req))</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> content[<span class="string">'r'</span>] == <span class="number">0</span> :</span><br><span class="line">        <span class="keyword">print</span> content[<span class="string">'msg'</span>]</span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">        <span class="keyword">if</span> data <span class="keyword">in</span> content: </span><br><span class="line">            <span class="keyword">print</span> content[<span class="string">'data'</span>]</span><br><span class="line">            exit()</span><br></pre></td></tr></table></figure>
</code></pre><p><br></p>
<p>知乎登陆的form里面有一个<code>_xsrf</code>字段，是防止xsrf攻击的，这个字段是在你请求www.zhihu.com的时候写到你cookie里面的，所以要先发一个到首页的请求拿到cookie里面的<code>_xsrf</code>值填充到form里面，然后拿到登陆的cookie。还有就是header里面的UA要伪造一个，否则会爬不到数据。</p>
<p>然后就是爬取用户回答的数据，我找了50个关注人数在1W以上或者是兴趣特征比较明显（例如回答主要集中在健身，历史）的用户进行的实验。解析页面用的是轻量级的BeautifulSoup库[2]，比较简单，容易上手。对于每个回答，只爬取了它的点赞数以及问题和回答的内容，因为发现有一些用户的回答是灌水的，也没有几个赞，所以根据点赞数不同，回答会占不同的权重，我在这里采用的是对点赞数开根号作为系数k来增加高赞回答的权重的。回答内容的分词用的是jieba库，它的基于TextRank算法的关键词抽取功能可以找出回答的关键字并且返回关键字的权重x，x*k就是一个回答中一个关键字的权重，用户所有回答中的x*k累加起来就是用户对这个关键字的权重，至此用户对于不同关键字的权重可以表示为行向量v，v中的第i项就是这个用户对第i个关键字的权重。下面是计算v的代码，其中wordcount就是该用户”<em>关键字——权重</em>“的一个dict.</p>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    <span class="comment">#被折叠的回答～</span></span><br><span class="line">    <span class="keyword">if</span> answer.textarea != <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment">#print answer</span></span><br><span class="line">        tag_as = answer.find_all(<span class="string">'a'</span>)</span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> tag_as:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment">#点赞数</span></span><br><span class="line">                <span class="keyword">if</span> a[<span class="string">'class'</span>] == <span class="string">'zm-item-vote-count js-expand js-vote-count'</span>:</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">'K'</span> <span class="keyword">in</span> a.string:</span><br><span class="line">                        k = (float(re.sub(<span class="string">'K'</span>, <span class="string">''</span>, a.string))*<span class="number">1000</span>) ** <span class="number">0.5</span></span><br><span class="line">                    <span class="keyword">elif</span> <span class="string">'W'</span> <span class="keyword">in</span> a.string:</span><br><span class="line">                        k = (float(re.sub(<span class="string">'W'</span>, <span class="string">''</span>, a.string))*<span class="number">10000</span>) ** <span class="number">0.5</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        k = float(a.string)**<span class="number">0.5</span></span><br><span class="line">                <span class="comment">#问题标题</span></span><br><span class="line">                <span class="keyword">if</span> a[<span class="string">'class'</span>] == <span class="string">'question_link'</span> :</span><br><span class="line">                    answer_string = a.string + <span class="string">' '</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="comment">#跳过无效的a标签</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">        <span class="comment">#回答</span></span><br><span class="line">        answer_string += answer.textarea.get_text()</span><br><span class="line">        answer_string = re.sub(<span class="string">'[A-Za-z0-9]'</span>, <span class="string">''</span>, answer_string)</span><br><span class="line">  </span><br><span class="line">        <span class="comment">#分词提取关键字</span></span><br><span class="line">        keywords = jieba.analyse.extract_tags(answer_string, <span class="number">100</span>, <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> keywords:</span><br><span class="line">            <span class="keyword">if</span> keyword[<span class="number">0</span>] <span class="keyword">in</span> wordcount:</span><br><span class="line">                wordcount[keyword[<span class="number">0</span>]] += keyword[<span class="number">1</span>]*k</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                wordcount[keyword[<span class="number">0</span>]] = keyword[<span class="number">1</span>]*k</span><br><span class="line">  </span><br><span class="line">    answer = answer.next_sibling.next_sibling</span><br><span class="line">    <span class="keyword">if</span> answer == <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
</code></pre><p><br></p>
<p>我们爬取完所有用户的回答并且计算出关键字的权重之后就可以获得一个<em>“用户——关键字”</em>的二维矩阵，当然，可能某个关键字会在绝大部分用户的回答中出现(<em>譬如”我”，”今天”..</em>)，或者某些关键字只出现在极个别的用户身上，这样的关键字对于聚类是无意义的，所以我们要对矩阵进行简单的处理(<em>我在这里用的是所有用户回答中出现概率是0.1到0.5的词进行聚类，我第一次实验对于50个用户的前100个回答进行关键词提取，这样筛选出来的关键字在5000个左右</em>)，再进行聚类操作。</p>
<p>在说K-Means算法之前，先简单说一下pearson相关度的计算，pearson相关度是用来比较两组数据相关程度的一种度量，除了这种方式常见的还有欧几里得距离，Jaccard系数等方法来计算相似程度，但是pearso相关度在我们这种数据不是很规范，每个人关键字都不一样，只有部分重复的情况下，拟合效果比较好，它返回1.0表示两组数据完全相同，0.0表示毫不相关。这是person相关度的公式:<img src="/img/1_1.jpg" alt="Alt text"></p>
<p>下面来说一下K-Means算法，K-Means算法是一个简单，快速的聚类算法，时间复杂度度不高。操作主要是以下几步：</p>
<ol>
<li>随机选择K个种子点作为初始类</li>
<li>计算每个用户数据和分类中心(<em>初始类中心就是种子点</em>)之间的pearson相关度，然后将这个数据放入相关度最大的一类中</li>
<li>计算第二步进行分类后的每一类的中心，再重复2，一直迭代到第2步聚类不发生变化为止</li>
</ol>
<p>这是我聚类迭代的代码，这个1000次的限制是之前老是死循环进去..数据量小的话十来次迭代应该就会出结果，这里的distance其实用上面的pearson相关度计算的。这里分类的每次迭代完新的分类的中心我就按平均值来计算的，其实也是有很多种方法来计算的。</p>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> times <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment">#聚类迭代</span></span><br><span class="line">    logging.info(<span class="string">'第%s次聚类迭代..'</span>%times)</span><br><span class="line">    bestmatches = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(k)]</span><br><span class="line">    sum_dis = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rows)):</span><br><span class="line">        bestmatch = <span class="number">0</span></span><br><span class="line">        min_dis = distance(rows[i], clusters[bestmatch])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(clusters)<span class="number">-1</span>):</span><br><span class="line">            d = distance(rows[i], clusters[j+<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> d &lt; min_dis:</span><br><span class="line">                bestmatch = j+<span class="number">1</span></span><br><span class="line">                min_dis = d</span><br><span class="line">  </span><br><span class="line">        bestmatches[bestmatch].append(i)</span><br><span class="line">        sum_dis += min_dis</span><br><span class="line">  </span><br><span class="line">    logging.info(<span class="string">'第%s次聚类all_dis为%s'</span>%(times, sum_dis))</span><br><span class="line">    <span class="keyword">if</span> lastmatches == bestmatches:</span><br><span class="line">        logging.info(<span class="string">'聚类迭代在第%s次结束'</span>%times)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    lastmatches = bestmatches</span><br><span class="line">  </span><br><span class="line">    <span class="comment">#第一轮分类结束，重新计算聚类中心，这里就用平均值来算</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        temp = [<span class="number">0.0</span>] * len(rows[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> len(bestmatches[i]) == <span class="number">0</span>:</span><br><span class="line">            clusters[i] = temp</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> bestmatches[i]:</span><br><span class="line">                temp = [ temp[item]+rows[j][item] <span class="keyword">for</span> item <span class="keyword">in</span> range(len(rows[<span class="number">0</span>])) ]</span><br><span class="line">            clusters[i] = [ temp[items]/len(bestmatches[i]) <span class="keyword">for</span> items <span class="keyword">in</span> range(len(rows[<span class="number">0</span>]))]</span><br></pre></td></tr></table></figure>
</code></pre><p><br><br>讲道理K-Means算法的缺陷也很多，我这里感受到最大的问题是聚类结果的K是自己指定的，但是往往你并不知道这些用户可以被聚类成多少类，当然还有收敛时间，初始种子点的选择的问题，我因为实验的数据量不大所以没有遇到。</p>
<p>在第一次实验的时候，50名用户的5000个关键字分成4类，只用了四五次迭代就收敛了。这是迭代的结果:</p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">negar-kordi guan-mao-12 quartz  tu-si-ji-da-lao-ye  chi-ban-ling-tai-lang</span><br><span class="line"></span><br><span class="line">fang-shi-yi-yue li-xu-ran-94    lu-shi-ya-62    li-jing-20-69   kun-kun aabbbccc    chen-ke-55-65   jennywang-94    tan-man-ru-28   shi-zho    ng-shen-jiao yusen   li-yang-89  song-de-90  mu-yi-happy luckystar   zhyang-liu  198Q    maigo   ma-qian-zu  so898   philippes   enostyle        wang-hong-hao-99    sijichun    SemitLee    rolnaersen  shi-wei-hu-bu-gui   amuro1230</span><br><span class="line"></span><br><span class="line">ji-xu-zhe-zhang douzishushu gao-ke-69   Ace1987 dai-jian-song   chenbailing lin-shuo-62 zhuoheng    trendol</span><br><span class="line"></span><br><span class="line">uc1874  Hemerocallis-Yu DKinBJ  wang-lin-83-53  tong-chi-bo-62  san-chong-bu-tong-de-hong-se    jump-sky    sparks</span><br></pre></td></tr></table></figure>
</code></pre><p><br><br>之前说了，选择用户的时候故意选择了几个健身话题比较关注的用户，结果都被分到了第三类中，第一类中有好几位都是外国人，还有一些国内外文化差异，历史之类的用户,第二类人太多我也没仔细看了…最后一类都是对最近如家女生遇袭，包贝尔婚礼之类女性话题回答比较多的，但是也有一两位对历史的回答比较多..不知道怎么分到了第四类。总的来说之前由于数据量,选取的样本不是很有代表性什么的问题，这次实验的结果还不是很清晰.而且实验结果可视化方面做的也不是很好.. 后续再完善吧～</p>
<blockquote>
<p>参考</p>
<ol>
<li><a href="https://github.com/fxsjy/jieba" target="_blank" rel="external">jieba库github地址</a></li>
<li><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" target="_blank" rel="external">BeautifulSoup库文档</a></li>
</ol>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/爬虫/">爬虫</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 李涵
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>